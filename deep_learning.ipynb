{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                                              \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import neuralNet as nn\n",
    "import utils as u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True)\n",
    "X = train_dataset.data.numpy()\n",
    "Y = train_dataset.targets.numpy()\n",
    "tX = test_dataset.data.numpy()\n",
    "tY = test_dataset.targets.numpy()\n",
    "\n",
    "a, b = 5, 6\n",
    "X0, Y0 = X.copy(), Y.copy()\n",
    "tX0, tY0 = tX.copy(), tY.copy()\n",
    "inds = (Y == a) | (Y == b)\n",
    "X, Y = X[inds], (Y[inds] == b).astype(int)\n",
    "t_inds = (tY == a) | (tY == b)\n",
    "tX, tY = tX[t_inds], (tY[t_inds] == b).astype(int)\n",
    "\n",
    "X = X.reshape([X.shape[0], -1]).astype(np.float64)\n",
    "tX = tX.reshape([tX.shape[0], -1]).astype(np.float64)\n",
    "Y = u.onehot(Y)\n",
    "tY = u.onehot(tY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 0.2121461448026564\n",
      "Epoch 2: training loss = 0.11634548539506029\n",
      "Epoch 3: training loss = 0.08511004267627463\n",
      "Epoch 4: training loss = 0.0670194244920911\n",
      "Epoch 5: training loss = 0.05575289254739551\n",
      "Epoch 6: training loss = 0.056816251344599854\n",
      "Epoch 7: training loss = 0.04399292606277416\n",
      "Epoch 8: training loss = 0.04140107624914126\n",
      "Epoch 9: training loss = 0.041871277901652\n",
      "Epoch 10: training loss = 0.03592128344056565\n"
     ]
    }
   ],
   "source": [
    "model = nn.NeuralNet(sizes=[X.shape[1], 10, 2], eta=1e-4, loss='categorical', nonLin=['sigmoid','softmax'], sf=0.05)\n",
    "it, E_ins = model.learn(X, Y, maxIters=10, trackE_in=True, print_stuff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1012.,   15.],\n",
       "       [  16.,  959.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def confusions(predictions, labels):\n",
    "    \"\"\"Return the confusions matrix\"\"\"\n",
    "    confusions = np.zeros([2, 2], np.float32)\n",
    "    bundled = zip(np.argmax(predictions, 1), np.argmax(labels, 1))\n",
    "    for predicted, actual in bundled:\n",
    "        confusions[predicted, actual] += 1\n",
    "    return confusions\n",
    "\n",
    "confusions(model.predict(tX), tY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, line1 = u.genF(zero_one=True) # We want labels to be 0/1 not -1/1\n",
    "X, Y = u.genData(f, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_plot_helper(model, X, Y, E_ins, line1, transform = lambda w: [*w[0][1:], w[0][0]]):\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(8,8))\n",
    "    u.plotE_ins(E_ins, axis=ax[0])\n",
    "    u.plotLine(*line1, axis=ax[1], label='Target')\n",
    "    u.plotLine(*transform(model.weights), color='g', \\\n",
    "                axis=ax[1], label='Hypothesis')\n",
    "    inds = (Y == 1)                                \n",
    "    ax[1].plot(X[inds, -2], X[inds, -1], 'b+')                   \n",
    "    ax[1].plot(X[~inds, -2], X[~inds, -1], 'r_')\n",
    "    ax[1].set_xlabel('X1'); ax[1].set_ylabel('X2')\n",
    "    ax[1].set_xlim([-1, 1]); ax[1].set_ylim([-1, 1])\n",
    "    ax[1].legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.NeuralNet(sizes=[3, 1], loss='log', nonLin='sigmoid')\n",
    "it, E_ins = model.learn(X, Y, maxIters=500, trackE_in=True)\n",
    "nn_plot_helper(model, X, Y, E_ins, line1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = lambda Y: np.hstack((Y[:, None], (1 - Y)[:, None]))\n",
    "def transform(w):\n",
    "    w_p = w[0][:, 0] - w[0][:, 1]\n",
    "    return [*w_p[1:], w_p[0]]\n",
    "    \n",
    "model2 = nn.NeuralNet(sizes=[3, 2], loss='categorical', nonLin='softmax')\n",
    "it, E_ins = model2.learn(X, one_hot(Y), maxIters=500, trackE_in=True)\n",
    "nn_plot_helper(model2, X, Y, E_ins, line1, transform = transform);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One variable function testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFunc = lambda x: np.sin(2 * x)\n",
    "\n",
    "inputs = np.linspace(-np.pi, np.pi, 200)\n",
    "outputs = testFunc(inputs)\n",
    "\n",
    "sizes = [1, 100, 100, 1]\n",
    "\n",
    "## For cosine, need different nonlinearity\n",
    "#model = nn.NeuralNet(sizes, nonLin='sigmoid')\n",
    "model = nn.NeuralNet(sizes, nonLin='tanh')\n",
    "\n",
    "it, E_ins = model.learn(inputs, outputs, maxIters=1000, trackE_in=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "u.plotE_ins(E_ins, axis=ax[0])\n",
    "model.quickPlot(inputs, outputs, axis=ax[1])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.uniform(-np.pi, np.pi, 20)\n",
    "outputs = testFunc(inputs)\n",
    "\n",
    "outputs2 = np.array([model.calculate(i) for i in inputs])\n",
    "outputs2 - outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Variable Function Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFunc = lambda x, y: np.cos(x + y)\n",
    "\n",
    "inputs = np.linspace(-np.pi / 2, np.pi / 2, 100)\n",
    "inputs = np.vstack((inputs / 3, inputs * 2 / 3)).T\n",
    "outputs = testFunc(inputs[:, 0], inputs[:, 1])\n",
    "\n",
    "sizes = [2, 100, 100, 1]\n",
    "\n",
    "model = nn.NeuralNet(sizes, nonLin='sigmoid')\n",
    "#model = nn.NeuralNet(sizes)\n",
    "\n",
    "it, E_ins = model.learn(inputs, outputs, maxIters=1000, trackE_in=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "u.plotE_ins(E_ins, axis=ax[0])\n",
    "\n",
    "ax[1].plot(np.sum(inputs, axis=1), [model.calculate(i) for i in inputs], color='g', label='Net Outputs')\n",
    "ax[1].plot(np.sum(inputs, axis=1), outputs, color='k', label='Real Outputs')\n",
    "ax[1].set_xlabel('input')\n",
    "ax[1].set_ylabel('output')\n",
    "ax[1].legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.uniform(-np.pi, np.pi, 20)\n",
    "inputs = np.vstack((inputs / 3, inputs * 2 / 3)).T\n",
    "outputs = testFunc(inputs[:, 0], inputs[:, 1])\n",
    "\n",
    "outputs2 = np.array([model.calculate(i) for i in inputs])\n",
    "outputs2 - outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR (doesn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFunc = lambda x: np.sum(np.atleast_2d(x), axis=1) % 2\n",
    "\n",
    "n_max = 1024\n",
    "nDigs = len(bin(n_max)[2:])\n",
    "inputs = np.array([[int(i) for i in list(bin(x)[2:].zfill(nDigs))] for x in range(n_max)])\n",
    "outputs = testFunc(inputs)\n",
    "\n",
    "sizes = [nDigs, 100, 1]\n",
    "\n",
    "model = nn.NeuralNet(sizes, loss='log', nonLin='sigmoid')\n",
    "\n",
    "it, E_ins = model.learn(inputs, outputs, maxIters=100, trackE_in=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "u.plotE_ins(E_ins, axis=ax)\n",
    "#model.quickPlot(inputs, outputs, axis=ax[1])\n",
    "\n",
    "fig.show()\n",
    "\n",
    "outputs2 = np.round(np.array([model.calculate(i) for i in inputs]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural",
   "language": "python",
   "name": "neural"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
