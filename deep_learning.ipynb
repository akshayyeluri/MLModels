{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                                              \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import neuralNet as nn\n",
    "import utils as u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True)\n",
    "X = train_dataset.data.numpy()\n",
    "Y = train_dataset.targets.numpy()\n",
    "tX = test_dataset.data.numpy()\n",
    "tY = test_dataset.targets.numpy()\n",
    "\n",
    "X, Y = X.reshape([X.shape[0], -1]), u.onehot(Y)\n",
    "tX, tY = tX.reshape([tX.shape[0], -1]), u.onehot(tY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshayyeluri/git/MLModels/neuralNet.py:27: RuntimeWarning: invalid value encountered in true_divide\n",
      "  'f' : lambda s :  np.exp(s) / np.exp(s).sum(), # softmax activation function\n",
      "/Users/akshayyeluri/git/MLModels/neuralNet.py:28: RuntimeWarning: invalid value encountered in true_divide\n",
      "  'df': lambda s :  np.diag(np.exp(s)/np.exp(s).sum()) - \\\n",
      "/Users/akshayyeluri/anaconda3/envs/neural/lib/python3.6/site-packages/numpy/core/numeric.py:1129: RuntimeWarning: invalid value encountered in multiply\n",
      "  return multiply(a.ravel()[:, newaxis], b.ravel()[newaxis, :], out)\n",
      "/Users/akshayyeluri/git/MLModels/neuralNet.py:29: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.outer(np.exp(s), np.exp(s)) / (np.exp(s).sum() ** 2)\n",
      "/Users/akshayyeluri/git/MLModels/neuralNet.py:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  'df': lambda t,y: -y / t\n",
      "/Users/akshayyeluri/git/MLModels/neuralNet.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  'df': lambda t,y: -y / t\n",
      "/Users/akshayyeluri/git/MLModels/neuralNet.py:40: RuntimeWarning: invalid value encountered in maximum\n",
      "  'f' : lambda s: np.maximum(0, s),\n",
      "/Users/akshayyeluri/git/MLModels/neuralNet.py:41: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  'df': lambda s: (s >= 0).astype(int)\n"
     ]
    }
   ],
   "source": [
    "model = nn.NeuralNet(sizes=[X.shape[1], 20, 10], loss='categorical', nonLin=['relu','softmax'])\n",
    "it, E_ins = model.learn(X, Y, maxIters=2, trackE_in=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, line1 = u.genF(zero_one=True) # We want labels to be 0/1 not -1/1\n",
    "X, Y = u.genData(f, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_plot_helper(model, X, Y, E_ins, line1, transform = lambda w: [*w[0][1:], w[0][0]]):\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(8,8))\n",
    "    u.plotE_ins(E_ins, axis=ax[0])\n",
    "    u.plotLine(*line1, axis=ax[1], label='Target')\n",
    "    u.plotLine(*transform(model.weights), color='g', \\\n",
    "                axis=ax[1], label='Hypothesis')\n",
    "    inds = (Y == 1)                                \n",
    "    ax[1].plot(X[inds, -2], X[inds, -1], 'b+')                   \n",
    "    ax[1].plot(X[~inds, -2], X[~inds, -1], 'r_')\n",
    "    ax[1].set_xlabel('X1'); ax[1].set_ylabel('X2')\n",
    "    ax[1].set_xlim([-1, 1]); ax[1].set_ylim([-1, 1])\n",
    "    ax[1].legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.NeuralNet(sizes=[3, 1], loss='log', nonLin='sigmoid')\n",
    "it, E_ins = model.learn(X, Y, maxIters=500, trackE_in=True)\n",
    "nn_plot_helper(model, X, Y, E_ins, line1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = lambda Y: np.hstack((Y[:, None], (1 - Y)[:, None]))\n",
    "def transform(w):\n",
    "    w_p = w[0][:, 0] - w[0][:, 1]\n",
    "    return [*w_p[1:], w_p[0]]\n",
    "    \n",
    "model2 = nn.NeuralNet(sizes=[3, 2], loss='categorical', nonLin='softmax')\n",
    "it, E_ins = model2.learn(X, one_hot(Y), maxIters=500, trackE_in=True)\n",
    "nn_plot_helper(model2, X, Y, E_ins, line1, transform = transform);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One variable function testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFunc = lambda x: np.sin(2 * x)\n",
    "\n",
    "inputs = np.linspace(-np.pi, np.pi, 200)\n",
    "outputs = testFunc(inputs)\n",
    "\n",
    "sizes = [1, 100, 100, 1]\n",
    "\n",
    "## For cosine, need different nonlinearity\n",
    "#model = nn.NeuralNet(sizes, nonLin='sigmoid')\n",
    "model = nn.NeuralNet(sizes, nonLin='tanh')\n",
    "\n",
    "it, E_ins = model.learn(inputs, outputs, maxIters=1000, trackE_in=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "u.plotE_ins(E_ins, axis=ax[0])\n",
    "model.quickPlot(inputs, outputs, axis=ax[1])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.uniform(-np.pi, np.pi, 20)\n",
    "outputs = testFunc(inputs)\n",
    "\n",
    "outputs2 = np.array([model.calculate(i) for i in inputs])\n",
    "outputs2 - outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Variable Function Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFunc = lambda x, y: np.cos(x + y)\n",
    "\n",
    "inputs = np.linspace(-np.pi / 2, np.pi / 2, 100)\n",
    "inputs = np.vstack((inputs / 3, inputs * 2 / 3)).T\n",
    "outputs = testFunc(inputs[:, 0], inputs[:, 1])\n",
    "\n",
    "sizes = [2, 100, 100, 1]\n",
    "\n",
    "model = nn.NeuralNet(sizes, nonLin='sigmoid')\n",
    "#model = nn.NeuralNet(sizes)\n",
    "\n",
    "it, E_ins = model.learn(inputs, outputs, maxIters=1000, trackE_in=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "u.plotE_ins(E_ins, axis=ax[0])\n",
    "\n",
    "ax[1].plot(np.sum(inputs, axis=1), [model.calculate(i) for i in inputs], color='g', label='Net Outputs')\n",
    "ax[1].plot(np.sum(inputs, axis=1), outputs, color='k', label='Real Outputs')\n",
    "ax[1].set_xlabel('input')\n",
    "ax[1].set_ylabel('output')\n",
    "ax[1].legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.uniform(-np.pi, np.pi, 20)\n",
    "inputs = np.vstack((inputs / 3, inputs * 2 / 3)).T\n",
    "outputs = testFunc(inputs[:, 0], inputs[:, 1])\n",
    "\n",
    "outputs2 = np.array([model.calculate(i) for i in inputs])\n",
    "outputs2 - outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR (doesn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFunc = lambda x: np.sum(np.atleast_2d(x), axis=1) % 2\n",
    "\n",
    "n_max = 1024\n",
    "nDigs = len(bin(n_max)[2:])\n",
    "inputs = np.array([[int(i) for i in list(bin(x)[2:].zfill(nDigs))] for x in range(n_max)])\n",
    "outputs = testFunc(inputs)\n",
    "\n",
    "sizes = [nDigs, 100, 1]\n",
    "\n",
    "model = nn.NeuralNet(sizes, loss='log', nonLin='sigmoid')\n",
    "\n",
    "it, E_ins = model.learn(inputs, outputs, maxIters=100, trackE_in=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "u.plotE_ins(E_ins, axis=ax)\n",
    "#model.quickPlot(inputs, outputs, axis=ax[1])\n",
    "\n",
    "fig.show()\n",
    "\n",
    "outputs2 = np.round(np.array([model.calculate(i) for i in inputs]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural",
   "language": "python",
   "name": "neural"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
